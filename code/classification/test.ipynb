{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.is_gpu_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the pickle object\n",
    "with open(\"../../dataset/pickles/LOC3.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Convert the data to a pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode the class labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['class_label'] = label_encoder.fit_transform(df['class_label'])\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_len = 0\n",
    "for i in df['lengths']:\n",
    "    max_len=max(max_len,len(i))\n",
    "\n",
    "# Pad the sequences to the maximum length\n",
    "sequences = np.array(df['lengths'])\n",
    "padded_sequences = np.zeros((len(sequences), max_len))\n",
    "for i, sequence in enumerate(sequences):\n",
    "    padded_sequences[i, :len(sequence)] = sequence\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, df['class_label'], test_size=0.2)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).to(torch.float32)\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "X_val = torch.from_numpy(X_val).to(torch.float32)\n",
    "y_val = torch.from_numpy(y_val.values).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "input_dim=0\n",
    "for i in X_train:\n",
    "    input_dim = max(input_dim,len(i))\n",
    "# input_dim = len(X_train[0])\n",
    "hidden_dim = 128\n",
    "output_dim = 1500\n",
    "learning_rate = 1e-3\n",
    "print(input_dim)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the dataloaders\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a custom dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.sequences[index]\n",
    "        label = self.labels[index]\n",
    "        return sequence, label\n",
    "\n",
    "# Define the model architecture\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        # simple 1d conv \n",
    "        self.conv1 = nn.Conv1d(1, 8, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.relu = nn.ReLU()        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(8*input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        # print(\"Size of FC: \", x.shape)\n",
    "        return self.fc(x)\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "rnn = RNN(input_dim, hidden_dim, output_dim, batch_size=64)\n",
    "rnn = rnn.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6663 Train Acc: 0.8241\n",
      "Test Accuracy of the model on the test sequences: 70.6747991093039 %\n",
      "Train Loss: 0.6593 Train Acc: 0.8249\n",
      "Test Accuracy of the model on the test sequences: 70.77161390260432 %\n",
      "Train Loss: 0.6503 Train Acc: 0.8286\n",
      "Test Accuracy of the model on the test sequences: 71.2879594668732 %\n",
      "Train Loss: 0.6440 Train Acc: 0.8294\n",
      "Test Accuracy of the model on the test sequences: 70.71352502662407 %\n",
      "Train Loss: 0.6362 Train Acc: 0.8308\n",
      "Test Accuracy of the model on the test sequences: 71.36218414173686 %\n",
      "Train Loss: 0.6323 Train Acc: 0.8310\n",
      "Test Accuracy of the model on the test sequences: 71.46222609481396 %\n",
      "Train Loss: 0.6233 Train Acc: 0.8332\n",
      "Test Accuracy of the model on the test sequences: 71.56226804789105 %\n",
      "Train Loss: 0.6183 Train Acc: 0.8340\n",
      "Test Accuracy of the model on the test sequences: 71.7881692322587 %\n",
      "Train Loss: 0.6128 Train Acc: 0.8357\n",
      "Test Accuracy of the model on the test sequences: 71.55904088811437 %\n",
      "Train Loss: 0.6080 Train Acc: 0.8358\n",
      "Test Accuracy of the model on the test sequences: 71.2556878691064 %\n",
      "Train Loss: 0.6034 Train Acc: 0.8372\n",
      "Test Accuracy of the model on the test sequences: 71.88821118533579 %\n",
      "Train Loss: 0.5997 Train Acc: 0.8386\n",
      "Test Accuracy of the model on the test sequences: 72.01084325684964 %\n",
      "Train Loss: 0.5931 Train Acc: 0.8404\n",
      "Test Accuracy of the model on the test sequences: 72.29806047697421 %\n",
      "Train Loss: 0.5919 Train Acc: 0.8400\n",
      "Test Accuracy of the model on the test sequences: 71.88175686578242 %\n",
      "Train Loss: 0.5853 Train Acc: 0.8415\n",
      "Test Accuracy of the model on the test sequences: 71.97534449930616 %\n",
      "Train Loss: 0.5818 Train Acc: 0.8430\n",
      "Test Accuracy of the model on the test sequences: 71.96566301997612 %\n",
      "Train Loss: 0.5773 Train Acc: 0.8437\n",
      "Test Accuracy of the model on the test sequences: 72.23997160099397 %\n",
      "Train Loss: 0.5751 Train Acc: 0.8449\n",
      "Test Accuracy of the model on the test sequences: 72.27224319876078 %\n",
      "Train Loss: 0.5715 Train Acc: 0.8446\n",
      "Test Accuracy of the model on the test sequences: 72.36260367250783 %\n",
      "Train Loss: 0.5664 Train Acc: 0.8459\n",
      "Test Accuracy of the model on the test sequences: 72.18833704456708 %\n",
      "Train Loss: 0.5626 Train Acc: 0.8472\n",
      "Test Accuracy of the model on the test sequences: 72.00438893729628 %\n",
      "Train Loss: 0.5626 Train Acc: 0.8468\n",
      "Test Accuracy of the model on the test sequences: 72.4336011875948 %\n",
      "Train Loss: 0.5580 Train Acc: 0.8482\n",
      "Test Accuracy of the model on the test sequences: 71.86884822667571 %\n",
      "Train Loss: 0.5550 Train Acc: 0.8485\n",
      "Test Accuracy of the model on the test sequences: 72.30774195630426 %\n",
      "Train Loss: 0.5531 Train Acc: 0.8487\n",
      "Test Accuracy of the model on the test sequences: 72.34324071384775 %\n",
      "Train Loss: 0.5476 Train Acc: 0.8502\n",
      "Test Accuracy of the model on the test sequences: 72.37228515183787 %\n",
      "Train Loss: 0.5459 Train Acc: 0.8501\n",
      "Test Accuracy of the model on the test sequences: 72.28515183786749 %\n",
      "Train Loss: 0.5431 Train Acc: 0.8515\n",
      "Test Accuracy of the model on the test sequences: 72.61432213508891 %\n",
      "Train Loss: 0.5415 Train Acc: 0.8520\n",
      "Test Accuracy of the model on the test sequences: 72.24965308032401 %\n",
      "Train Loss: 0.5380 Train Acc: 0.8525\n",
      "Test Accuracy of the model on the test sequences: 72.15606544680027 %\n",
      "Train Loss: 0.5352 Train Acc: 0.8524\n",
      "Test Accuracy of the model on the test sequences: 72.75309000548617 %\n",
      "Train Loss: 0.5339 Train Acc: 0.8530\n",
      "Test Accuracy of the model on the test sequences: 72.70468260883597 %\n",
      "Train Loss: 0.5309 Train Acc: 0.8534\n",
      "Test Accuracy of the model on the test sequences: 72.88540355633008 %\n",
      "Train Loss: 0.5292 Train Acc: 0.8536\n",
      "Test Accuracy of the model on the test sequences: 72.83376899990319 %\n",
      "Train Loss: 0.5255 Train Acc: 0.8547\n",
      "Test Accuracy of the model on the test sequences: 72.55946041888534 %\n",
      "Train Loss: 0.5261 Train Acc: 0.8551\n",
      "Test Accuracy of the model on the test sequences: 72.84022331945654 %\n",
      "Train Loss: 0.5218 Train Acc: 0.8556\n",
      "Test Accuracy of the model on the test sequences: 72.51750734178849 %\n",
      "Train Loss: 0.5196 Train Acc: 0.8563\n",
      "Test Accuracy of the model on the test sequences: 72.94349243231032 %\n",
      "Train Loss: 0.5193 Train Acc: 0.8568\n",
      "Test Accuracy of the model on the test sequences: 72.50459870268178 %\n",
      "Train Loss: 0.5139 Train Acc: 0.8570\n",
      "Test Accuracy of the model on the test sequences: 72.7304998870494 %\n",
      "Train Loss: 0.5158 Train Acc: 0.8571\n",
      "Test Accuracy of the model on the test sequences: 72.91767515409688 %\n",
      "Train Loss: 0.5114 Train Acc: 0.8579\n",
      "Test Accuracy of the model on the test sequences: 72.76277148481621 %\n",
      "Train Loss: 0.5112 Train Acc: 0.8580\n",
      "Test Accuracy of the model on the test sequences: 72.7821344434763 %\n",
      "Train Loss: 0.5078 Train Acc: 0.8580\n",
      "Test Accuracy of the model on the test sequences: 72.9305837932036 %\n",
      "Train Loss: 0.5067 Train Acc: 0.8592\n",
      "Test Accuracy of the model on the test sequences: 72.81763320101979 %\n",
      "Train Loss: 0.5071 Train Acc: 0.8581\n",
      "Test Accuracy of the model on the test sequences: 73.08871462226095 %\n",
      "Train Loss: 0.5044 Train Acc: 0.8593\n",
      "Test Accuracy of the model on the test sequences: 72.94349243231032 %\n",
      "Train Loss: 0.5030 Train Acc: 0.8587\n",
      "Test Accuracy of the model on the test sequences: 72.98867266918386 %\n",
      "Train Loss: 0.5012 Train Acc: 0.8597\n",
      "Test Accuracy of the model on the test sequences: 73.11453190047439 %\n",
      "Train Loss: 0.4992 Train Acc: 0.8610\n",
      "Test Accuracy of the model on the test sequences: 73.10162326136766 %\n",
      "Train Loss: 0.4963 Train Acc: 0.8618\n",
      "Test Accuracy of the model on the test sequences: 72.69500112950593 %\n",
      "Train Loss: 0.4964 Train Acc: 0.8611\n",
      "Test Accuracy of the model on the test sequences: 73.02417142672734 %\n",
      "Train Loss: 0.4945 Train Acc: 0.8616\n",
      "Test Accuracy of the model on the test sequences: 73.02739858650402 %\n",
      "Train Loss: 0.4940 Train Acc: 0.8621\n",
      "Test Accuracy of the model on the test sequences: 72.57236905799206 %\n",
      "Train Loss: 0.4933 Train Acc: 0.8617\n",
      "Test Accuracy of the model on the test sequences: 72.4981443831284 %\n",
      "Train Loss: 0.4923 Train Acc: 0.8620\n",
      "Test Accuracy of the model on the test sequences: 73.02739858650402 %\n",
      "Train Loss: 0.4892 Train Acc: 0.8622\n",
      "Test Accuracy of the model on the test sequences: 72.99189982896053 %\n",
      "Train Loss: 0.4903 Train Acc: 0.8616\n",
      "Test Accuracy of the model on the test sequences: 72.77890728369961 %\n",
      "Train Loss: 0.4853 Train Acc: 0.8635\n",
      "Test Accuracy of the model on the test sequences: 72.89185787588343 %\n",
      "Train Loss: 0.4849 Train Acc: 0.8641\n",
      "Test Accuracy of the model on the test sequences: 72.66272953173912 %\n",
      "Train Loss: 0.4857 Train Acc: 0.8627\n",
      "Test Accuracy of the model on the test sequences: 72.80149740213638 %\n",
      "Train Loss: 0.4833 Train Acc: 0.8637\n",
      "Test Accuracy of the model on the test sequences: 72.84022331945654 %\n",
      "Train Loss: 0.4831 Train Acc: 0.8640\n",
      "Test Accuracy of the model on the test sequences: 72.88863071610676 %\n",
      "Train Loss: 0.4820 Train Acc: 0.8640\n",
      "Test Accuracy of the model on the test sequences: 72.98867266918386 %\n",
      "Train Loss: 0.4801 Train Acc: 0.8644\n",
      "Test Accuracy of the model on the test sequences: 73.10807758092103 %\n",
      "Train Loss: 0.4786 Train Acc: 0.8646\n",
      "Test Accuracy of the model on the test sequences: 73.21134669377481 %\n",
      "Train Loss: 0.4770 Train Acc: 0.8647\n",
      "Test Accuracy of the model on the test sequences: 72.90476651499016 %\n",
      "Train Loss: 0.4755 Train Acc: 0.8647\n",
      "Test Accuracy of the model on the test sequences: 73.01771710717398 %\n",
      "Train Loss: 0.4763 Train Acc: 0.8656\n",
      "Test Accuracy of the model on the test sequences: 72.90153935521347 %\n",
      "Train Loss: 0.4762 Train Acc: 0.8649\n",
      "Test Accuracy of the model on the test sequences: 73.04353438538742 %\n",
      "Train Loss: 0.4736 Train Acc: 0.8658\n",
      "Test Accuracy of the model on the test sequences: 72.61109497531223 %\n",
      "Train Loss: 0.4721 Train Acc: 0.8664\n",
      "Test Accuracy of the model on the test sequences: 72.96930971052377 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m sequences \u001b[39m=\u001b[39m sequences\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m sequences \u001b[39m=\u001b[39m sequences\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[39m=\u001b[39m rnn(sequences)\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(sequences.shape)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(outputs.shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(labels.shape)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/765/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[66], line 30\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     28\u001b[0m     \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m---> 30\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m     31\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     32\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/765/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/765/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/765/lib/python3.11/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "# optimizer = optimizer.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    rnn.train()\n",
    "    for sequences, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        labels = labels.to(device)\n",
    "        sequences = sequences.to(device)\n",
    "        sequences = sequences.unsqueeze(1)\n",
    "        outputs = rnn(sequences)\n",
    "        \n",
    "        # print(sequences.shape)\n",
    "        # print(outputs.shape)\n",
    "        # print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * sequences.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() /len(train_dataset)\n",
    "    print('Train Loss: {:.4f} Train Acc: {:.4f} %'.format(epoch_loss, epoch_acc*100))\n",
    "    \n",
    "    # test the model\n",
    "    rnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for sequences, labels in val_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            sequences = sequences.unsqueeze(1)\n",
    "            outputs = rnn(sequences)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy of the model on the test sequences: {} %'.format(100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.7257 Val Acc: 0.6621\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "rnn.eval()\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in val_loader:\n",
    "        labels = labels.to(device)\n",
    "        sequences = sequences.to(device)\n",
    "        sequences = sequences.unsqueeze(1)\n",
    "        outputs = rnn(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * sequences.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "epoch_loss = running_loss / len(val_dataset)\n",
    "epoch_acc = running_corrects.double() / len(val_dataset)\n",
    "print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.predict(test_data)\n",
    "print(\"Accuracy: \", accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "707",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
